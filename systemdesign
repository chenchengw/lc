reference:https://www.evernote.com/shard/s576/sh/7e58b450-1abe-43a8-bf82-fbf07f1db13c/049802174415b418a2e65f75b744ab72
http://www.cnblogs.com/LBSer/p/3310455.html

good one: http://systemdesigns.blogspot.com/2015/12/ood-blackjack.html
inverted index: http://blog.csdn.net/hguisu/article/details/7969757

http://www.1point3acres.com/bbs/thread-210083-1-1.html

http://blog.csdn.net/v_july_v/article/details/6685962

1. TinyUrl https://segmentfault.com/a/1190000006140476

Requirements:

DAU: 
Read: 100M
Write: 10M

QPS:
Read: 1K

10M*100Bytes = 1GB /day => 1TB/ 3 years

Service:
GET:/shortUrl
  response: 301 (longUrl)

POST:
  request body: longUrl
  response: 200 (shortUrl)
 
void redirect(String shortUrl);

Option 1:
把long_url用md5/sha1哈希
md5把一个string转化成128位二进制数，一般用32位十六进制数(16byte)表示：
http://site.douban.com/chuan -> c93a360dc7f3eb093ab6e304db516653
sha1把一个string转化成160位二进制数，一般用40位十六进制数(20byte)表示：
http://site.douban.com/chuan -> dff85871a72c73c3eae09e39ffe97aea63047094
这两个算法可以保证哈希值分布很随机，但是冲突是不可避免的，任何一个哈希算法都不可避免有冲突。
优点：简单，可以根据long_url直接生成；假设一个url中一个char占两个字节，平均长度为30的话，原url占大小60byte,hash之后要16byte。我们可以取md5的前6位,这样就更节省。
缺点：难以保证哈希算法没有冲突
解决冲突方案：1.拿(long_url + timestamp)来哈希；2.冲突的话，重试(timestamp会变，会生成新的hash)
综上，流量不多时，可行；但是，当url超过了假设1 billion的时候，冲突会非常多，效率非常低。

Option 2:
将六位的short_url看做是一个62进制数(0-9,a-z,A-Z)，可以表示62^6=570亿个数。整个互联网的网页数在trillion级别，即一万亿这个级别。6位足够。
每个short_url对应一个十进制整数，这个整数就可以是sql数据库中的自增id，即auto_increment_id。

public class Codec {
    //reference https://www.bittiger.io/blog/post/JJcLtcFc8MWzSmbdW
    final static String dict = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ";
    Map<String, Integer> long2idx = new HashMap<>();
    Map<Integer, String> idx2long = new HashMap<>();   
    
    // Encodes a URL to a shortened URL.
    public String encode(String longUrl) {
        if (!long2idx.containsKey(longUrl)) {
            long2idx.put(longUrl, long2idx.size()+1);
            idx2long.put(long2idx.size(), longUrl);
        }
        String shortUrl = encode62(long2idx.get(longUrl));
        return "http://tinyurl.com/" + shortUrl;
    }

    // Decodes a shortened URL to its original URL.
    public String decode(String shortUrl) {
        int idx = decode62(shortUrl);
        if (idx2long.containsKey(idx)) return idx2long.get(idx);
        else return null;
    }
    
    String encode62(int number) {
        StringBuilder sb = new StringBuilder();
        while (number > 0) {
            char c = dict.charAt(number%62);
            sb.insert(0, c);
            number /= 62;
        }
        return sb.toString();
    }

    int decode62(String s) {
        int val = 0;
        for (int i = 0; i < s.length(); i++) {
            val *= 62;
            val += dict.indexOf(s.charAt(i));
        }
        return val;
    }
}


Storage:

MySQL: schema 
long | String
id   | longUrl

index longUrl so that it can be looked up easily

Scale:
1. Build cache and redirect the read traffic to cache

POST 
longUrl.hashCode%62 (consistent hashing) -> machine B -> store longUrl -> id convert to 3b7eZ 

Implementation of consistent hashing: http://www.tom-e-white.com//2007/11/consistent-hashing.html
(key word: treeMap, tailTree, round to first key)

return B3b7eZ

GET
B3b7eZ -> machine B -> 3b7eZ convert to id -> longUrl 

新增一台机器 -> 找原来机器里负责range(0-61)最大的机器 -> 将其range减半 -> 把一半放到新增机器上

继续优化？

中国的db放到中国，美国的db放到美国。
用地域信息作为sharding key，比如中国网站都放到0开头的，美国网站都放在1开头的。
加一个新功能custom url怎么做？

单独建一张表，存custom_url <--> long_url
当查询时，先从custom表里查，再从url表里查。
注意，千万不要在url表里插一列custom，这样这列大部分的值为空。


2. Typeahead search/autocomplete

Requirements: 
1. how many suggestions we should provide? 5
2. do we need to account for spelling mistakes? yes -> edit distance
3. personalization needed or just globally?

QPS:

Total: 2-4 billion like Google 
5 words each query, each word 5 characters -> 25*4 = 200 billion words upper bound perday
storage needs: 500m(new searches)*25bytes = 12.5GB per day -> 12TB in 3 years

latency and availability are very important to us

Service:
GET:/words
request body {words}
return: 200K with 5 suggestions

Within the service
getTopSuggestion(word)

data structure: trie
A bruteforce way is to scan all the nodes in the subtree and find the 5 most frequent. 
Improvement is to store 5 most frequent words in every node of the trie

How would a typical write work in this trie?
A: So, now whenever we get an actual search term, we will traverse down to the node corresponding to it and 
increase its frequency. But wait, we are not done yet. We store the top 5 queries in each node. Its possible 
that this particular search query jumped into the top 5 queries of a few other nodes. 
We need to update the top 5 queries of those nodes then. How do we do it then?
Truthfully, we need to know the frequencies of the top 5 queries ( of every node in the path from root to the node ) 
to decide if this query becomes a part of the top 5.
There are 2 ways we could achieve this.

    Along with the top 5 on every node, we also store their frequency. Anytime, a node’s frequency gets updated, 
    we traverse back from the node to its parent till we reach the root. For every parent, we check if the current query
    is part of the top 5. If so, we replace the corresponding frequency with the updated frequency. If not, we check 
    if the current query’s frequency is high enough to be a part of the top 5. If so, we update the top 5 with frequency.
    On every node, we store the top pointer to the end node of the 5 most frequent queries ( pointers instead of the text ). 
    The update process would involve comparing the current query’s frequency with the 5th lowest node’s frequency and update 
    the node pointer with the current query pointer if the new frequency is greater.
    
  One optimization we can do is: Sample the writes to update the frequency of the words

Another approach we can take is to update the frequency offline:
 we can have an offline hashmap which keeps maintaining a map from query to frequency. Its only when the frequency becomes a 
 multiple of a threshold that we go and update the query in the trie with the new frequency. The hashmap being a separate 
 datastore would not collide with the actual trie for reads.
 
 Storage:
 
 Single Machine? 50TB, No!
 The number of shards could very well be more than the number of branches on first level(26). 
 We will need to be more intelligent than just sharding on first level. Imbalance, two levels, 
 but we need zookeeper to keep track of the mapping of branch to servers.
 
 Availability is very important, we can maintain multiple replica of each shard and an update goes to all replicas. 
 The read can go to multiple replicas (not necessarily all) and uses the first response it gets. 
 If a replica goes down, reads and writes continue to work fine as there are other replicas to serve the queries. 
 
 trie can be design as multi level hashtable. thus, the key value data store is the choice. 
 
 
3. Design a messaging system

Requirement (daily)
DAU: 100M, 20 message per user => 20*100M/86400 ~= 20K => peak QPS 100K
messages sent: 100M*20 = 2B, each message 100 bytes, 200GB


CAP Theory:
latency is important
consistency is important
availablity is okay

Service:

Message service:
1. send a message to another user 
2. fetch most recent converstaions

Real-time service:
push message to receiver

Storage: 

    Message Table:
    messageId, content, date_created, last_updated
    
    Messages Table:
    from_user, to_user, messageId

    Thread Table:
    user_id (primary key), thread_id (created_user_id + timestamp) + last_updated(secondary idx), participant_ids, date_created, last_updated 

    POST
    createGroup get thread_id
     request body {
         creatorUserId
         repientUserIds
     }

     POST
     send message
     request body {
         senderUserId,
         receiverUserId,
         messageContent
     } 

    return 200K

    1. store the message to message table.
        If the message is sent to an existing thread, the thread id is fetched and store to message table.
        Otherwise, create a new thread with all participants recorded and store the message under the thread_id created.

    2. send the message to all participants except sender in the corresponding thread. 
    

Scale:
Group chat? Push 500 messages even if only 1 or two are online?
add a channel service
When user is logged in, message service identify the of the user channel, and subscribe to the channel
when user is dropped, push service unsubscribe the user from the channel

message service got message, find the channel users belongs to.
channel service identify online users and push the message through push service


Q: How to check status? online or offline?
heartbeat every 3-5 seconds , if we haven't received the heartbeat from a user, we think it is offline.

4. Design Netflix

让我设计一个 Netflix/Spotify，follow up 很多 比如如何限制同一个用户多处登录不能同时看一个资源，如何实现根据用户的网速调整清晰度，怎么热门推荐等等。
对于设计登录系统，首先我们先要设计基本的login系统，首先支持Register/login，就需要一个基本的用户表，如果要支持更复杂一点的登录系统，包括verification，ban，inactive，removed，就需要一个status来记录每个user的状态。
如果还要支持用户可以从多个设备登录，不同的设备会有不同的sessionid，但是却有相同的user——id，如果不想要同一个用户可以在多个设备上播放同样的资源，我们就要记录每个用户每个session正在play的资源，然后保证每个session不能play相同的song_ID或者video_id。如果要根据网络速度调整清晰度，我们就需要先测量网络的情况，让client端ping一下服务器，根据收到的ping再向服务器请求不同清晰度的资源。资源本身不应该存在服务器上，可以存在离客户端很近的CDN上。
然后要看播放器的类型，如果是app的话，这个app会需要向DNSserver请求我们服务器的地址，然后我们服务器向clientload网页（如果是从浏览器的话，如果是app的话，就不需要load网页，直接去请求源地址就可以了）。然后网页再加载播放器（flash或者HTML5的），最后再请求源地址，源地址应该是放在附近的CDN上面的，所以很快。
接下来是推荐系统，假设问怎么样设计一个推荐系统，推荐给用户Top10most frequentplayed。最简单的用MinHeap，把所有的play过的video或者audio都记录一个frequency，这个记录可以放在内存里（可以是cluster的这种比如redis）这样便于快速的更新和存取。我们希望这个工作放在worker里做，这样可以不影响整个系统的throughput。
当worker完成了工作以后，就可以Update所有的client，这个工作也可以由一个worker来做，这里我们可以用push也可以用pull，也可以两者相结合，push的话只需要push给在线用户，pull的话对于刚上线的用户来更新最新结果。
外排序就是假设内存不够用，那么我们就将所有的数据分成小块，然后每个小块都可以放进内存里排序，排好了序的这些小块再进行mergesort。就得到了所有数据的排序结果。
还有一个followup就是因为有很多用户在同时播放视频文件，有可能同时有很多人在看同一个视频，那么这个视频的freq就会有很多+1，怎么样保证所有的+1都记录下来呢？我们可以用一个aggregator，专门记录这种update，等update到了一定的数量或者一定时间做一次batch的update。

5. Design Uber
 前两个问题不是重点，主要是在建立编程模型的适合需要更多的抽象和信息来对business logic提供支持。system design的重点是第三个问题，选择合适的partition。
 Uber用了Google S2 lib对地图进行做划分. s2可以把地球上的细微到平方厘米的面积进行唯一的编号. S2的最高划分level是30。但是uber用不到这么小的划分，所以选择了level 12的划分。
 这个划分的scale是3-6 平方公里。因为越细小的划分会导致request area被大量的cells 覆盖，导致产生大量的empty response，会给服务器增加负担。
 每一个cell有一个ID，所以就用这些ID产生了一个geospatial index。图2中的紫色圆圈就是一个rider的所在区域，然后要找到所有覆盖这个区域的cells。
 现在rider发出一个请求，然后会在所有覆盖的cells里面找available的driver。dispatch server 有了rider的位置和附近的driver就能通过历史交通信息估算出driver pick up rider的时间了，然后根据ETA进行排序，选择哪一个driver去接rider。

Ringpop所基于的SWIN本身只是一个Gossip协议。其中核心Distributed Hashing部分使用的可能是CHORD（Wiki https://en.wikipedia.org/wiki/Chord_(peer-to-peer)有比较详细的解释），
一种Distributed Hash Table (DHT)经典算法（参考论文https://www.cs.cornell.edu/~asdas/research/dsn02-swim.pdf）。
目标是在保证Consistent Hashing性质的同时，在log(N)时间内找到某一个Key（即cell）所在的server，N是server的总数。
有趣的是CHORD的数据结构恰好是一个ring（可能是ringpop名字的由来）。

每一个集群节点，在一个周期中（不断重复此周期），随机选择另一个集群节点作为探测节点。
Ø  PING该探测节点，如果回复（收到ACK），该周期活动结束，否则
Ø  随机选择K个集群节点，让这些节点分别PING探测节点，如果K个中有任意一个回复能PING通探测节点，该周期活动结束，否则
Ø  标记改随机节点死亡
b，改节点向集群其他节点“广播”探测节点死亡。
～～～通俗讲法：
有一群人在一起聚会，A随机选择到一个人B，并想知道他是否已经离开。他高声喊B的名字，恰好B去上洗手间了，没有听到；
于是他又随机的找了10个人，让他们帮忙找B是否还在。悲剧的是这10个人都太投入于聚会，随便找了一下没找到B，就告诉A说，B已经离开了。
最后A确认B已经离开，并且大声告诉所有人这一情况。（这其中错误的把没有离开的B检测为离开了，因此算法结果存在false negative；
不过假设B真的走了，他的离开一定会被检测到）

在CHORD中，所有的集群节点和所有的Query（通过hash）都被影射到同一个（环形）空间中。每个Query的存储和请求都由“环中”顺时针方向离它最近的集群节点负责。
每一个集群节点都存储一个叫做FingerTable的Hash Table（每一条记录的Key是环上的一个点，Value是Hash值大于这个点的机器中的第一个），
任何节点在收到一个不由它负责的Query后都将Query转发到FingerTable中离Query最近的节点。（CHORD保证在O(log(N))时间内把Query转发给负责它的节点）。

6. web crawler system
https://www.zhihu.com/question/20899988
bloom filter: https://llimllib.github.io/bloomfilter-tutorial/ to avoid loop




 







