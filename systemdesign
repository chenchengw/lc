reference:https://www.evernote.com/shard/s576/sh/7e58b450-1abe-43a8-bf82-fbf07f1db13c/049802174415b418a2e65f75b744ab72
http://www.cnblogs.com/LBSer/p/3310455.html

good one: http://systemdesigns.blogspot.com/2015/12/ood-blackjack.html
inverted index: http://blog.csdn.net/hguisu/article/details/7969757

http://www.1point3acres.com/bbs/thread-210083-1-1.html

http://blog.csdn.net/v_july_v/article/details/6685962

1. TinyUrl https://segmentfault.com/a/1190000006140476

Requirements:

DAU: 
Read: 100M
Write: 10M

QPS:
Read: 1K

10M*100Bytes = 1GB /day => 1TB/ 3 years

Service:
GET:/shortUrl
  response: 301 (longUrl)

POST:
  request body: longUrl
  response: 200 (shortUrl)
 
void redirect(String shortUrl);

Option 1:
把long_url用md5/sha1哈希
md5把一个string转化成128位二进制数，一般用32位十六进制数(16byte)表示：
http://site.douban.com/chuan -> c93a360dc7f3eb093ab6e304db516653
sha1把一个string转化成160位二进制数，一般用40位十六进制数(20byte)表示：
http://site.douban.com/chuan -> dff85871a72c73c3eae09e39ffe97aea63047094
这两个算法可以保证哈希值分布很随机，但是冲突是不可避免的，任何一个哈希算法都不可避免有冲突。
优点：简单，可以根据long_url直接生成；假设一个url中一个char占两个字节，平均长度为30的话，原url占大小60byte,hash之后要16byte。我们可以取md5的前6位,这样就更节省。
缺点：难以保证哈希算法没有冲突
解决冲突方案：1.拿(long_url + timestamp)来哈希；2.冲突的话，重试(timestamp会变，会生成新的hash)
综上，流量不多时，可行；但是，当url超过了假设1 billion的时候，冲突会非常多，效率非常低。

Option 2:
将六位的short_url看做是一个62进制数(0-9,a-z,A-Z)，可以表示62^6=570亿个数。整个互联网的网页数在trillion级别，即一万亿这个级别。6位足够。
每个short_url对应一个十进制整数，这个整数就可以是sql数据库中的自增id，即auto_increment_id。

public class Codec {
    //reference https://www.bittiger.io/blog/post/JJcLtcFc8MWzSmbdW
    final static String dict = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ";
    Map<String, Integer> long2idx = new HashMap<>();
    Map<Integer, String> idx2long = new HashMap<>();   
    
    // Encodes a URL to a shortened URL.
    public String encode(String longUrl) {
        if (!long2idx.containsKey(longUrl)) {
            long2idx.put(longUrl, long2idx.size()+1);
            idx2long.put(long2idx.size(), longUrl);
        }
        String shortUrl = encode62(long2idx.get(longUrl));
        return "http://tinyurl.com/" + shortUrl;
    }

    // Decodes a shortened URL to its original URL.
    public String decode(String shortUrl) {
        int idx = decode62(shortUrl);
        if (idx2long.containsKey(idx)) return idx2long.get(idx);
        else return null;
    }
    
    String encode62(int number) {
        StringBuilder sb = new StringBuilder();
        while (number > 0) {
            char c = dict.charAt(number%62);
            sb.insert(0, c);
            number /= 62;
        }
        return sb.toString();
    }

    int decode62(String s) {
        int val = 0;
        for (int i = 0; i < s.length(); i++) {
            val *= 62;
            val += dict.indexOf(s.charAt(i));
        }
        return val;
    }
}


Storage:

MySQL: schema 
long | String
id   | longUrl

index longUrl so that it can be looked up easily

Scale:
1. Build cache and redirect the read traffic to cache

POST 
longUrl.hashCode%62 (consistent hashing) -> machine B -> store longUrl -> id convert to 3b7eZ 

Implementation of consistent hashing: http://www.tom-e-white.com//2007/11/consistent-hashing.html
(key word: treeMap, tailTree, round to first key)

return B3b7eZ

GET
B3b7eZ -> machine B -> 3b7eZ convert to id -> longUrl 

新增一台机器 -> 找原来机器里负责range(0-61)最大的机器 -> 将其range减半 -> 把一半放到新增机器上

继续优化？

中国的db放到中国，美国的db放到美国。
用地域信息作为sharding key，比如中国网站都放到0开头的，美国网站都放在1开头的。
加一个新功能custom url怎么做？

单独建一张表，存custom_url <--> long_url
当查询时，先从custom表里查，再从url表里查。
注意，千万不要在url表里插一列custom，这样这列大部分的值为空。

问题1：new use case扩展－how to implement time-limited service?
state在这里就有用武之地了，超时的就清空

安全性的缺点，如果ID生成对应的tinyURL的算法过于简单，则容易被人套出以前被map过的长url。（举个例子，auto increase id）；
解决办法是用 Feistel cipher 来对increment id进行变形，产生伪随机字符排列。

1、可以增加server数量，配合load balancer来解决
2、可以通过实现cache来做到（比如LRU）
3、也可以使用Message queue+异步式操作解决
4、还有个Rate limiter（也在这阶段的案例库中）
In computer networks, rate limiting is used to control the rate of traffic sent or received by a network interface controller。假如server最高只能接受50w/s的qps，一旦冲击到了100w，服务器崩溃就可能挂掉。Rate Limiter会把超过50w／s的请求全拒绝掉。
5、 “全内存”方式
如果数据查询都要进入硬盘查询的话，速度会很慢，此处提到的全内存方式，提倡将映射数据存到内存中，这样查询速度就会变快很多。有以下两种方式：
1、先写硬盘，再写内存（牺牲性能，获得一致性）
2、先写内存，返回，再写硬盘（保证性能，暂时不保证一致，但可以最终获得一致性，用准确度换取性能）
可以用redis/memcache，轻量级的请求memcache就够了，复杂/重级服务用redis。


2. Typeahead search/autocomplete

Requirements: 
1. how many suggestions we should provide? 5
2. do we need to account for spelling mistakes? yes -> edit distance
3. personalization needed or just globally?

QPS:

Total: 2-4 billion like Google 
5 words each query, each word 5 characters -> 25*4 = 200 billion words upper bound perday
storage needs: 500m(new searches)*25bytes = 12.5GB per day -> 12TB in 3 years

latency and availability are very important to us

Service:
GET:/words
request body {words}
return: 200K with 5 suggestions

Within the service
getTopSuggestion(word)

data structure: trie
A bruteforce way is to scan all the nodes in the subtree and find the 5 most frequent. 
Improvement is to store 5 most frequent words in every node of the trie

How would a typical write work in this trie?
A: So, now whenever we get an actual search term, we will traverse down to the node corresponding to it and 
increase its frequency. But wait, we are not done yet. We store the top 5 queries in each node. Its possible 
that this particular search query jumped into the top 5 queries of a few other nodes. 
We need to update the top 5 queries of those nodes then. How do we do it then?
Truthfully, we need to know the frequencies of the top 5 queries ( of every node in the path from root to the node ) 
to decide if this query becomes a part of the top 5.
There are 2 ways we could achieve this.

    Along with the top 5 on every node, we also store their frequency. Anytime, a node’s frequency gets updated, 
    we traverse back from the node to its parent till we reach the root. For every parent, we check if the current query
    is part of the top 5. If so, we replace the corresponding frequency with the updated frequency. If not, we check 
    if the current query’s frequency is high enough to be a part of the top 5. If so, we update the top 5 with frequency.
    On every node, we store the top pointer to the end node of the 5 most frequent queries ( pointers instead of the text ). 
    The update process would involve comparing the current query’s frequency with the 5th lowest node’s frequency and update 
    the node pointer with the current query pointer if the new frequency is greater.
    
  One optimization we can do is: Sample the writes to update the frequency of the words

Another approach we can take is to update the frequency offline:
 we can have an offline hashmap which keeps maintaining a map from query to frequency. Its only when the frequency becomes a 
 multiple of a threshold that we go and update the query in the trie with the new frequency. The hashmap being a separate 
 datastore would not collide with the actual trie for reads.
 
 Storage:
 
 Single Machine? 50TB, No!
 The number of shards could very well be more than the number of branches on first level(26). 
 We will need to be more intelligent than just sharding on first level. Imbalance, two levels, 
 but we need zookeeper to keep track of the mapping of branch to servers.
 
 Availability is very important, we can maintain multiple replica of each shard and an update goes to all replicas. 
 The read can go to multiple replicas (not necessarily all) and uses the first response it gets. 
 If a replica goes down, reads and writes continue to work fine as there are other replicas to serve the queries. 
 
 trie can be design as multi level hashtable. thus, the key value data store is the choice. 
 
 
3. Design a messaging system

Requirement (daily)
DAU: 100M, 20 message per user => 20*100M/86400 ~= 20K => peak QPS 100K
messages sent: 100M*20 = 2B, each message 100 bytes, 200GB


CAP Theory:
latency is important
consistency is important
availablity is okay

Service:

Message service:
1. send a message to another user 
2. fetch most recent converstaions

Real-time service:
push message to receiver

Storage: 

    Message Table:
    messageId, content, date_created, last_updated
    
    Messages Table:
    from_user, to_user, messageId

    Thread Table:
    user_id (primary key), thread_id (created_user_id + timestamp) + last_updated(secondary idx), participant_ids, date_created, last_updated 

    POST
    createGroup get thread_id
     request body {
         creatorUserId
         repientUserIds
     }

     POST
     send message
     request body {
         senderUserId,
         receiverUserId,
         messageContent
     } 

    return 200K

    1. store the message to message table.
        If the message is sent to an existing thread, the thread id is fetched and store to message table.
        Otherwise, create a new thread with all participants recorded and store the message under the thread_id created.

    2. send the message to all participants except sender in the corresponding thread. 
    

Scale:
Group chat? Push 500 messages even if only 1 or two are online?
add a channel service
When user is logged in, message service identify the of the user channel, and subscribe to the channel
when user is dropped, push service unsubscribe the user from the channel

message service got message, find the channel users belongs to.
channel service identify online users and push the message through push service


Q: How to check status? online or offline?
heartbeat every 3-5 seconds , if we haven't received the heartbeat from a user, we think it is offline.

4. Design Netflix

让我设计一个 Netflix/Spotify，follow up 很多 比如如何限制同一个用户多处登录不能同时看一个资源，如何实现根据用户的网速调整清晰度，怎么热门推荐等等。
对于设计登录系统，首先我们先要设计基本的login系统，首先支持Register/login，就需要一个基本的用户表，如果要支持更复杂一点的登录系统，包括verification，ban，inactive，removed，就需要一个status来记录每个user的状态。
如果还要支持用户可以从多个设备登录，不同的设备会有不同的sessionid，但是却有相同的user——id，如果不想要同一个用户可以在多个设备上播放同样的资源，我们就要记录每个用户每个session正在play的资源，然后保证每个session不能play相同的song_ID或者video_id。如果要根据网络速度调整清晰度，我们就需要先测量网络的情况，让client端ping一下服务器，根据收到的ping再向服务器请求不同清晰度的资源。资源本身不应该存在服务器上，可以存在离客户端很近的CDN上。
然后要看播放器的类型，如果是app的话，这个app会需要向DNSserver请求我们服务器的地址，然后我们服务器向clientload网页（如果是从浏览器的话，如果是app的话，就不需要load网页，直接去请求源地址就可以了）。然后网页再加载播放器（flash或者HTML5的），最后再请求源地址，源地址应该是放在附近的CDN上面的，所以很快。
接下来是推荐系统，假设问怎么样设计一个推荐系统，推荐给用户Top10most frequentplayed。最简单的用MinHeap，把所有的play过的video或者audio都记录一个frequency，这个记录可以放在内存里（可以是cluster的这种比如redis）这样便于快速的更新和存取。我们希望这个工作放在worker里做，这样可以不影响整个系统的throughput。
当worker完成了工作以后，就可以Update所有的client，这个工作也可以由一个worker来做，这里我们可以用push也可以用pull，也可以两者相结合，push的话只需要push给在线用户，pull的话对于刚上线的用户来更新最新结果。
外排序就是假设内存不够用，那么我们就将所有的数据分成小块，然后每个小块都可以放进内存里排序，排好了序的这些小块再进行mergesort。就得到了所有数据的排序结果。
还有一个followup就是因为有很多用户在同时播放视频文件，有可能同时有很多人在看同一个视频，那么这个视频的freq就会有很多+1，怎么样保证所有的+1都记录下来呢？我们可以用一个aggregator，专门记录这种update，等update到了一定的数量或者一定时间做一次batch的update。

5. Design Uber
 前两个问题不是重点，主要是在建立编程模型的适合需要更多的抽象和信息来对business logic提供支持。system design的重点是第三个问题，选择合适的partition。
 Uber用了Google S2 lib对地图进行做划分. s2可以把地球上的细微到平方厘米的面积进行唯一的编号. S2的最高划分level是30。但是uber用不到这么小的划分，所以选择了level 12的划分。
 这个划分的scale是3-6 平方公里。因为越细小的划分会导致request area被大量的cells 覆盖，导致产生大量的empty response，会给服务器增加负担。
 每一个cell有一个ID，所以就用这些ID产生了一个geospatial index。图2中的紫色圆圈就是一个rider的所在区域，然后要找到所有覆盖这个区域的cells。
 现在rider发出一个请求，然后会在所有覆盖的cells里面找available的driver。dispatch server 有了rider的位置和附近的driver就能通过历史交通信息估算出driver pick up rider的时间了，然后根据ETA进行排序，选择哪一个driver去接rider。

Ringpop所基于的SWIN本身只是一个Gossip协议。其中核心Distributed Hashing部分使用的可能是CHORD（Wiki https://en.wikipedia.org/wiki/Chord_(peer-to-peer)有比较详细的解释），
一种Distributed Hash Table (DHT)经典算法（参考论文https://www.cs.cornell.edu/~asdas/research/dsn02-swim.pdf）。
目标是在保证Consistent Hashing性质的同时，在log(N)时间内找到某一个Key（即cell）所在的server，N是server的总数。
有趣的是CHORD的数据结构恰好是一个ring（可能是ringpop名字的由来）。

每一个集群节点，在一个周期中（不断重复此周期），随机选择另一个集群节点作为探测节点。
Ø  PING该探测节点，如果回复（收到ACK），该周期活动结束，否则
Ø  随机选择K个集群节点，让这些节点分别PING探测节点，如果K个中有任意一个回复能PING通探测节点，该周期活动结束，否则
Ø  标记改随机节点死亡
b，改节点向集群其他节点“广播”探测节点死亡。
～～～通俗讲法：
有一群人在一起聚会，A随机选择到一个人B，并想知道他是否已经离开。他高声喊B的名字，恰好B去上洗手间了，没有听到；
于是他又随机的找了10个人，让他们帮忙找B是否还在。悲剧的是这10个人都太投入于聚会，随便找了一下没找到B，就告诉A说，B已经离开了。
最后A确认B已经离开，并且大声告诉所有人这一情况。（这其中错误的把没有离开的B检测为离开了，因此算法结果存在false negative；
不过假设B真的走了，他的离开一定会被检测到）

在CHORD中，所有的集群节点和所有的Query（通过hash）都被影射到同一个（环形）空间中。每个Query的存储和请求都由“环中”顺时针方向离它最近的集群节点负责。
每一个集群节点都存储一个叫做FingerTable的Hash Table（每一条记录的Key是环上的一个点，Value是Hash值大于这个点的机器中的第一个），
任何节点在收到一个不由它负责的Query后都将Query转发到FingerTable中离Query最近的节点。（CHORD保证在O(log(N))时间内把Query转发给负责它的节点）。

6. web crawler system

initial_page =https://www.zhihu.com/question/20899988
url_queue = Queue.Queue()
seen = set()

seen.insert(initial_page)
url_queue.put(initial_page)

while(True): #一直进行直到海枯石烂
    if url_queue.size()>0:
        current_url = url_queue.get()    #拿出队例中第一个的url
        store(current_url)               #把这个url代表的网页存储好
        for next_url in extract_urls(current_url): #提取把这个url里链向的url
            if next_url not in seen:      
                seen.put(next_url)
                url_queue.put(next_url)
    else:
        break


1) 查重
当页面多了的时候，第一个会遇到问题就是查重的问题，也就是检查这个页面是不是已经爬过。如果用BloomFilter 用来查重，复杂度会从O(NlogN)  简化到 O(N)。
bloom filter: https://llimllib.github.io/bloomfilter-tutorial/ to avoid loop



如何入门 Python 爬虫？
关注问题写回答
Python
爬虫 (计算机网络)
如何入门 Python 爬虫？
关注者
44545
被浏览
2412603
关注问题写回答
12 条评论
分享
邀请回答

193 个回答
默认排序
谢科
谢科
用python分布式地爬过豆瓣/Twitter Search
收录于 编辑推荐知乎周刊 · 10819 人赞同了该回答
“入门”是良好的动机，但是可能作用缓慢。如果你手里或者脑子里有一个项目，那么实践起来你会被目标驱动，而不会像学习模块一样慢慢学习。

另外如果说知识体系里的每一个知识点是图里的点，依赖关系是边的话，那么这个图一定不是一个有向无环图。因为学习A的经验可以帮助你学习B。因此，你不需要学习怎么样“入门”，因为这样的“入门”点根本不存在！你需要学习的是怎么样做一个比较大的东西，在这个过程中，你会很快地学会需要学会的东西的。当然，你可以争论说需要先懂python，不然怎么学会python做爬虫呢？但是事实上，你完全可以在做这个爬虫的过程中学习python :D

看到前面很多答案都讲的“术”——用什么软件怎么爬，那我就讲讲“道”和“术”吧——爬虫怎么工作以及怎么在python实现。

先长话短说summarize一下：
你需要学习
基本的爬虫工作原理
基本的http抓取工具，scrapy
Bloom Filter: Bloom Filters by Example
如果需要大规模网页抓取，你需要学习分布式爬虫的概念。其实没那么玄乎，你只要学会怎样维护一个所有集群机器能够有效分享的分布式队列就好。最简单的实现是python-rq: https://github.com/nvie/rq
rq和Scrapy的结合：darkrho/scrapy-redis · GitHub
后续处理，网页析取(grangier/python-goose · GitHub)，存储(Mongodb)

以下是短话长说：

说说当初写的一个集群爬下整个豆瓣的经验吧。

1）首先你要明白爬虫怎样工作。
想象你是一只蜘蛛，现在你被放到了互联“网”上。那么，你需要把所有的网页都看一遍。怎么办呢？没问题呀，你就随便从某个地方开始，比如说人民日报的首页，这个叫initial pages，用$表示吧。

在人民日报的首页，你看到那个页面引向的各种链接。于是你很开心地从爬到了“国内新闻”那个页面。太好了，这样你就已经爬完了俩页面（首页和国内新闻）！暂且不用管爬下来的页面怎么处理的，你就想象你把这个页面完完整整抄成了个html放到了你身上。

突然你发现， 在国内新闻这个页面上，有一个链接链回“首页”。作为一只聪明的蜘蛛，你肯定知道你不用爬回去的吧，因为你已经看过了啊。所以，你需要用你的脑子，存下你已经看过的页面地址。这样，每次看到一个可能需要爬的新链接，你就先查查你脑子里是不是已经去过这个页面地址。如果去过，那就别去了。

好的，理论上如果所有的页面可以从initial page达到的话，那么可以证明你一定可以爬完所有的网页。

那么在python里怎么实现呢？
很简单
import Queue

initial_page = "http://www.renminribao.com"

url_queue = Queue.Queue()
seen = set()

seen.insert(initial_page)
url_queue.put(initial_page)

while(True): #一直进行直到海枯石烂
    if url_queue.size()>0:
        current_url = url_queue.get()    #拿出队例中第一个的url
        store(current_url)               #把这个url代表的网页存储好
        for next_url in extract_urls(current_url): #提取把这个url里链向的url
            if next_url not in seen:      
                seen.put(next_url)
                url_queue.put(next_url)
    else:
        break
写得已经很伪代码了。

所有的爬虫的backbone都在这里，下面分析一下为什么爬虫事实上是个非常复杂的东西——搜索引擎公司通常有一整个团队来维护和开发。

2）效率
如果你直接加工一下上面的代码直接运行的话，你需要一整年才能爬下整个豆瓣的内容。更别说Google这样的搜索引擎需要爬下全网的内容了。

问题出在哪呢？需要爬的网页实在太多太多了，而上面的代码太慢太慢了。设想全网有N个网站，那么分析一下判重的复杂度就是N*log(N)，因为所有网页要遍历一次，而每次判重用set的话需要log(N)的复杂度。OK，OK，我知道python的set实现是hash——不过这样还是太慢了，至少内存使用效率不高。

通常的判重做法是怎样呢？Bloom Filter. 简单讲它仍然是一种hash的方法，但是它的特点是，它可以使用固定的内存（不随url的数量而增长）以O(1)的效率判定url是否已经在set中。可惜天下没有白吃的午餐，它的唯一问题在于，如果这个url不在set中，BF可以100%确定这个url没有看过。但是如果这个url在set中，它会告诉你：这个url应该已经出现过，不过我有2%的不确定性。注意这里的不确定性在你分配的内存足够大的时候，可以变得很小很少。一个简单的教程:Bloom Filters by Example

注意到这个特点，url如果被看过，那么可能以小概率重复看一看（没关系，多看看不会累死）。但是如果没被看过，一定会被看一下（这个很重要，不然我们就要漏掉一些网页了！）。 [IMPORTANT: 此段有问题，请暂时略过]


好，现在已经接近处理判重最快的方法了。另外一个瓶颈——你只有一台机器。不管你的带宽有多大，只要你的机器下载网页的速度是瓶颈的话，那么你只有加快这个速度。用一台机子不够的话——用很多台吧！当然，我们假设每台机子都已经进了最大的效率——使用多线程（python的话，多进程吧）。

3）集群化抓取
爬取豆瓣的时候，我总共用了100多台机器昼夜不停地运行了一个月。想象如果只用一台机子你就得运行100个月了...

那么，假设你现在有100台机器可以用，怎么用python实现一个分布式的爬取算法呢？

我们把这100台中的99台运算能力较小的机器叫作slave，另外一台较大的机器叫作master，那么回顾上面代码中的url_queue，如果我们能把这个queue放到这台master机器上，所有的slave都可以通过网络跟master联通，每当一个slave完成下载一个网页，就向master请求一个新的网页来抓取。而每次slave新抓到一个网页，就把这个网页上所有的链接送到master的queue里去。同样，bloom filter也放到master上，但是现在master只发送确定没有被访问过的url给slave。Bloom Filter放到master的内存里，而被访问过的url放到运行在master上的Redis里，这样保证所有操作都是O(1)。（至少平摊是O(1)，Redis的访问效率见:LINSERT – Redis)


考虑如何用python实现：
在各台slave上装好scrapy，那么各台机子就变成了一台有抓取能力的slave，在master上装好Redis和rq用作分布式队列。

2) 下载速度
第二个bottleneck在页面的下载速度上。这里提到解决下载速度慢的方法就是 用master - slave结构。
代码于是写成
#slave.py

current_url = request_from_master()
to_send = []
for next_url in extract_urls(current_url):
    to_send.append(next_url)

store(current_url);
send_to_master(to_send)

#master.py
distributed_queue = DistributedQueue()
bf = BloomFilter()

initial_pages = "www.renmingribao.com"

while(True):
    if request == 'GET':
        if distributed_queue.size()>0:
            send(distributed_queue.get())
        else:
            break
    elif request == 'POST':
        bf.put(request.url)

这样就很好解决了一个资源浪费的问题。在这幅图里面画了task table的架构。这里面有一些列还是值得思考的。
- Priority
比如说priority是用来划分爬虫的优先级的。可以把要用来爬list的优先级提高。把要来爬page的优先级降低。
- State
还有state在这里包含 done, working, new 这几个状态。这个可以用于管理，这个也可以用来做verification。比如说一个爬虫的状态是done了。那就应该是爬过了。但是如果没有得到应有的数据，及link里面的内容。我就可以拿它来查是不是那个爬虫坏了。
- Timestamp
最后面还有两个时间。这样还能做到定时爬取的效果，原因可能在于要对一个网站做到爬虫友好。

scheduler multi threaded
1. if no new tasks sleep for 1 second and check again
2. mutex
3. semaphore

拿任务,送回页面 分成了两类 sender和receiver这样一组爬虫结构 就需要两个大的控制结构就好。

 







